{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Content Similarity Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doosti@chapman.edu/.conda/envs/ctopics/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-01 15:37:02.065407: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-01 15:37:02.149619: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-01 15:37:02.150718: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-01 15:37:04.498861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from top2vec import Top2Vec\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/doosti@chapman.edu/projects/Facebook/top2vec/\"\n",
    "DATA_PATH = os.path.join(PATH,'data')\n",
    "model_name = \"top2vec_deeplearn_distiluse_notoken_2024-06-27.model\"\n",
    "model_path = os.path.join(DATA_PATH, model_name)\n",
    "textdata_file = \"data_all_text.csv\"\n",
    "textdata_path = os.path.join(DATA_PATH, textdata_file)\n",
    "data_file = \"pooled_us_aug2020.dta\"\n",
    "data_path = os.path.join(DATA_PATH, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 3936\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = Top2Vec.load(model_path)\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f\"Number of topics: {len(topic_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data Size: (820099, 8)\n",
      "Data Size: (220033, 151)\n"
     ]
    }
   ],
   "source": [
    "# load text data\n",
    "textdata = pd.read_csv(textdata_path, low_memory=False)\n",
    "print(f\"Text Data Size: {textdata.shape}\")\n",
    "# load data\n",
    "data = pd.read_stata(data_path)\n",
    "print(f\"Data Size: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 220033 rows, 138397 rows have video_id in textdata\n",
      "Out of 220033 rows, 220033 rows have creator_id in textdata\n",
      "Out of 34028 rows, 17337 rows have sponsor_id in textdata\n",
      "Number of rows with non-missing similarity: 17273\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "# control for video ids\n",
    "print(f\"Out of {data.shape[0]} rows, {data[data.video_id.astype(int).isin(textdata.video_id.values)].shape[0]} rows have video_id in textdata\")\n",
    "# control for creator ids\n",
    "print(f\"Out of {data.shape[0]} rows, {data[data.creator_id.isin(textdata.creator_id.values)].shape[0]} rows have creator_id in textdata\")\n",
    "# control for sponosr ids\n",
    "print(f\"Out of {data[data.sponsored==1].shape[0]} rows, {data[(data.sponsored==1) & (data.sponsor_id.isin(textdata.creator_id))].shape[0]} rows have sponsor_id in textdata\")\n",
    "# control for current similarity metrics\n",
    "print(f\"Number of rows with non-missing similarity: {data[data.similarity.notnull()].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    print(data[~(data.video_id.astype(int)).isin(textdata.video_id.values)][['video_id','creator_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to check the corresponsing creator by video id\n",
    "text_creator = dict(zip(textdata.video_id, textdata.creator_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18847/1523541730.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['new_id'] = data.video_id.astype(int)\n"
     ]
    }
   ],
   "source": [
    "# correcting for video id\n",
    "data['new_id'] = data.video_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138397\n"
     ]
    }
   ],
   "source": [
    "# those who match\n",
    "mask = data.new_id.isin(textdata.video_id.values)\n",
    "print(mask.sum())\n",
    "for i,row in data[mask][['video_id','creator_id']].iterrows():\n",
    "    creator = row.creator_id\n",
    "    video_id = row.video_id\n",
    "    if text_creator[video_id]!=creator:\n",
    "        print(f\"video_id: {video_id}, creator_id: {creator}, text_creator: {text_creator[video_id]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one up or one down\n",
    "new_ids = []\n",
    "for i, row in data[~mask][['new_id','creator_id']].iterrows():\n",
    "    creator = row.creator_id\n",
    "    video_id = row.new_id\n",
    "    if text_creator.get(video_id,None)==creator:\n",
    "        raise \"Something wrong happened!\" # shouldn't happen\n",
    "    elif text_creator.get(video_id+1,None)==creator:\n",
    "        new_ids.append(video_id+1)\n",
    "    elif text_creator.get(video_id-1,None)==creator:\n",
    "        new_ids.append(video_id-1)\n",
    "    else:\n",
    "        new_ids.append(video_id)\n",
    "data.loc[~mask,'new_id'] = new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220033"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data.new_id.isin(textdata.video_id.values).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sponsor Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m creator_embeddings_names \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m creator, vectors \u001b[38;5;129;01min\u001b[39;00m creator_embeddings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# two dict by id and name\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     creator_embeddings_names[\u001b[43mcreators_id2name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreator\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m vectors\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(creator_embeddings))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(creator_embeddings_names))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "# calculate creator embeddings\n",
    "document_vectors = model.document_vectors\n",
    "creators = textdata['creator_id'].values\n",
    "\n",
    "# Group document vectors by author\n",
    "creator_to_vectors = defaultdict(list)\n",
    "\n",
    "for creator, vector in zip(creators, document_vectors):\n",
    "    creator_to_vectors[creator].append(vector)\n",
    "\n",
    "creator_embeddings = {}\n",
    "\n",
    "for creator, vectors in creator_to_vectors.items():\n",
    "    # Calculate the mean vector (centroid) for each author\n",
    "    creator_embeddings[creator] = np.mean(vectors, axis=0)\n",
    "    # l2 normalization\n",
    "    creator_embeddings[creator] /= np.linalg.norm(creator_embeddings[creator])\n",
    "\n",
    "print(len(creator_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the creator embeddings\n",
    "creator_embeddings_path = os.path.join(DATA_PATH, 'creator_embeddings.pkl')\n",
    "with open(creator_embeddings_path, 'wb') as f:\n",
    "    pickle.dump(creator_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4596\n"
     ]
    }
   ],
   "source": [
    "# load the creator embeddings\n",
    "creator_embeddings_path = os.path.join(DATA_PATH, 'creator_embeddings.pkl')\n",
    "with open(creator_embeddings_path, 'rb') as f:\n",
    "    creator_embeddings = pickle.load(f)\n",
    "\n",
    "print(len(creator_embeddings))\n",
    "\n",
    "creators_id2name = dict(zip(textdata['creator_id'], textdata['creator_name']))\n",
    "creator_embeddings_names = {}\n",
    "\n",
    "for creator, vectors in creator_embeddings.items():\n",
    "    # two dict by id and name\n",
    "    creator_embeddings_names[creators_id2name[creator]] = vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity for creators\n",
    "def get_similar_creators(creator, creator_embeddings, top_n=5):\n",
    "    creator_embedding = creator_embeddings[creator]\n",
    "    similarities = {}\n",
    "    for key, value in creator_embeddings.items():\n",
    "        similarities[key] = cosine_similarity([creator_embedding], [value])[0][0]\n",
    "    similar_creators = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return similar_creators[:top_n]\n",
    "\n",
    "def print_similar_creators(creator, similar_creators):\n",
    "    print(f\"Creators similar to {creator}:\")\n",
    "    for i, (creator, similarity) in enumerate(similar_creators):\n",
    "        print(f\"{i+1:2d}. ({similarity:4.2f}) {creator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity between a document and a creator\n",
    "def get_sponsorship_similarity(document_vector, creator_embedding):\n",
    "    return cosine_similarity([document_vector], [creator_embedding])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dictionary of document embeddings\n",
    "document_vectors = model.document_vectors\n",
    "video_ids = textdata['video_id'].values\n",
    "\n",
    "#video_to_vector = dict(zip(video_ids, document_vectors))\n",
    "video_to_vector = {}\n",
    "for i, video_id in enumerate(video_ids):\n",
    "    video_to_vector[video_id] = document_vectors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688593767965037 KFeKzFHyhL Metro PCS 0.7335498332977295\n",
      "1162201353815813 KFeKzFHyhL Metro PCS 0.7335498332977295\n"
     ]
    }
   ],
   "source": [
    "# calculate similarity between each document and each creator\n",
    "new_metric = []\n",
    "for i, row in data.iterrows():\n",
    "    video_id = row['new_id']\n",
    "    creator_id = row['creator_id']\n",
    "    sponsor_id = row['sponsor_id']\n",
    "    sponsor_name = row['sponsor_name']\n",
    "    if row['sponsored'] == 0:\n",
    "        new_metric.append(np.nan)\n",
    "    elif (video_id not in video_to_vector):\n",
    "        new_metric.append(np.nan)\n",
    "    elif (sponsor_id not in creator_embeddings):\n",
    "        if sponsor_name not in creator_embeddings_names:\n",
    "            new_metric.append(np.nan)\n",
    "            if ~np.isnan(row['similarity']):\n",
    "                print(video_id, sponsor_id, sponsor_name, row['similarity'])\n",
    "        else:\n",
    "            new_metric.append(get_sponsorship_similarity(video_to_vector[video_id], creator_embeddings_names[sponsor_name]))\n",
    "    else:\n",
    "        new_metric.append(get_sponsorship_similarity(video_to_vector[video_id], creator_embeddings[sponsor_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['similarity_new'] = new_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity        17273\n",
      "similarity_new    17343\n",
      "dtype: int64\n",
      "                similarity  similarity_new\n",
      "similarity        1.000000        0.332057\n",
      "similarity_new    0.332057        1.000000\n",
      "         similarity  similarity_new\n",
      "count  17273.000000    17343.000000\n",
      "mean       0.683697        0.338835\n",
      "std        0.075628        0.160717\n",
      "min        0.504807       -0.197543\n",
      "25%        0.628146        0.223607\n",
      "50%        0.675194        0.331094\n",
      "75%        0.732961        0.452767\n",
      "max        0.954277        0.853619\n"
     ]
    }
   ],
   "source": [
    "print(data[['similarity','similarity_new']].notnull().sum())\n",
    "print(data[['similarity','similarity_new']].corr())\n",
    "print(data[['similarity','similarity_new']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "data.to_csv(os.path.join(DATA_PATH,\"pooled_us_jul2024.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18847/3119501987.py:1: PossiblePrecisionLoss: \n",
      "Column converted from int64 to float64, and some data are outside of the lossless\n",
      "conversion range. This may result in a loss of precision in the saved data.\n",
      "\n",
      "  data.to_stata(\"pooled_us_jul2024.dta\", write_index=False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nFixed width strings in Stata .dta files are limited to 244 (or fewer)\ncharacters.  Column 'themes' does not satisfy this restriction. Use the\n'version=117' parameter to write the newer (Stata 13 and later) format.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_stata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpooled_us_jul2024.dta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ctopics/lib/python3.8/site-packages/pandas/core/frame.py:2672\u001b[0m, in \u001b[0;36mDataFrame.to_stata\u001b[0;34m(self, path, convert_dates, write_index, byteorder, time_stamp, data_label, variable_labels, version, convert_strl, compression, storage_options, value_labels)\u001b[0m\n\u001b[1;32m   2668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m118\u001b[39m:\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;66;03m# Specifying the version is only supported for UTF8 (118 or 119)\u001b[39;00m\n\u001b[1;32m   2670\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m version\n\u001b[0;32m-> 2672\u001b[0m writer \u001b[38;5;241m=\u001b[39m \u001b[43mstatawriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbyteorder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyteorder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_stamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_stamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2684\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2686\u001b[0m writer\u001b[38;5;241m.\u001b[39mwrite_file()\n",
      "File \u001b[0;32m~/.conda/envs/ctopics/lib/python3.8/site-packages/pandas/io/stata.py:2350\u001b[0m, in \u001b[0;36mStataWriter.__init__\u001b[0;34m(self, fname, data, convert_dates, write_index, byteorder, time_stamp, data_label, variable_labels, compression, storage_options, value_labels)\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_converted_names: \u001b[38;5;28mdict\u001b[39m[Hashable, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2349\u001b[0m \u001b[38;5;66;03m# attach nobs, nvars, data, varlist, typlist\u001b[39;00m\n\u001b[0;32m-> 2350\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[1;32m   2353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ctopics/lib/python3.8/site-packages/pandas/io/stata.py:2634\u001b[0m, in \u001b[0;36mStataWriter._prepare_pandas\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2631\u001b[0m \u001b[38;5;66;03m# Verify object arrays are strings and encode to bytes\u001b[39;00m\n\u001b[1;32m   2632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_strings()\n\u001b[0;32m-> 2634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_formats_and_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[38;5;66;03m# set the given format for the datetime cols\u001b[39;00m\n\u001b[1;32m   2637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_dates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ctopics/lib/python3.8/site-packages/pandas/io/stata.py:2573\u001b[0m, in \u001b[0;36mStataWriter._set_formats_and_types\u001b[0;34m(self, dtypes)\u001b[0m\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtyplist: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, dtype \u001b[38;5;129;01min\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 2573\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmtlist\u001b[38;5;241m.\u001b[39mappend(\u001b[43m_dtype_to_default_stata_fmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtyplist\u001b[38;5;241m.\u001b[39mappend(_dtype_to_stata_type(dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[col]))\n",
      "File \u001b[0;32m~/.conda/envs/ctopics/lib/python3.8/site-packages/pandas/io/stata.py:2217\u001b[0m, in \u001b[0;36m_dtype_to_default_stata_fmt\u001b[0;34m(dtype, column, dta_version, force_strl)\u001b[0m\n\u001b[1;32m   2215\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%9s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2216\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2217\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(excessive_string_length_error\u001b[38;5;241m.\u001b[39mformat(column\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   2218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mmax\u001b[39m(itemsize, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2219\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n",
      "\u001b[0;31mValueError\u001b[0m: \nFixed width strings in Stata .dta files are limited to 244 (or fewer)\ncharacters.  Column 'themes' does not satisfy this restriction. Use the\n'version=117' parameter to write the newer (Stata 13 and later) format.\n"
     ]
    }
   ],
   "source": [
    "data.to_stata(os.path.join(DATA_PATH,\"pooled_us_jul2024.dta\"), write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctopics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
