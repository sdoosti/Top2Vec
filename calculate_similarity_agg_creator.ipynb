{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Content Similarity Metric (Partnership Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity score is aggregated at the creator level instead of video level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doosti@chapman.edu/.conda/envs/ctopics/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-06 12:29:01.623692: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-06 12:29:01.707104: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-06 12:29:01.708294: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-06 12:29:04.002115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from top2vec import Top2Vec\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/doosti@chapman.edu/projects/Facebook/top2vec/\"\n",
    "DATA_PATH = os.path.join(PATH,'data')\n",
    "model_name = \"top2vec_deeplearn_distiluse_notoken_2024-06-27.model\"\n",
    "model_path = os.path.join(DATA_PATH, model_name)\n",
    "textdata_file = \"data_all_text.csv\"\n",
    "textdata_path = os.path.join(DATA_PATH, textdata_file)\n",
    "data_file = \"pooled_us_aug2020.dta\"\n",
    "data_path = os.path.join(DATA_PATH, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 3936\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = Top2Vec.load(model_path)\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f\"Number of topics: {len(topic_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data Size: (820099, 8)\n",
      "Data Size: (220033, 151)\n"
     ]
    }
   ],
   "source": [
    "# load text data\n",
    "textdata = pd.read_csv(textdata_path, low_memory=False)\n",
    "print(f\"Text Data Size: {textdata.shape}\")\n",
    "# load data\n",
    "data = pd.read_stata(data_path)\n",
    "print(f\"Data Size: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 220033 rows, 138397 rows have video_id in textdata\n",
      "Out of 220033 rows, 220033 rows have creator_id in textdata\n",
      "Out of 34028 rows, 17337 rows have sponsor_id in textdata\n",
      "Number of rows with non-missing similarity: 17273\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "# control for video ids\n",
    "print(f\"Out of {data.shape[0]} rows, {data[data.video_id.astype(int).isin(textdata.video_id.values)].shape[0]} rows have video_id in textdata\")\n",
    "# control for creator ids\n",
    "print(f\"Out of {data.shape[0]} rows, {data[data.creator_id.isin(textdata.creator_id.values)].shape[0]} rows have creator_id in textdata\")\n",
    "# control for sponosr ids\n",
    "print(f\"Out of {data[data.sponsored==1].shape[0]} rows, {data[(data.sponsored==1) & (data.sponsor_id.isin(textdata.creator_id))].shape[0]} rows have sponsor_id in textdata\")\n",
    "# control for current similarity metrics\n",
    "print(f\"Number of rows with non-missing similarity: {data[data.similarity.notnull()].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  video_id  creator_id\n",
      "72773  10100462134286216.0  FKAjsn3tbe\n",
      "72774  10100970241156152.0  yE4rDdIZMX\n",
      "72775  10100985156021640.0  yE4rDdIZMX\n",
      "72776  10100985156111460.0  yE4rDdIZMX\n",
      "72791  10106352251543832.0  kOZJsafwog\n",
      "...                    ...         ...\n",
      "220023 10214037392314108.0  AM7pEDQGGW\n",
      "220025 10214058075868220.0  HaF9jQYWeA\n",
      "220027 10214156295289160.0  C2OyuzQa8y\n",
      "220028 10214255689048236.0  hdbSjOZX99\n",
      "220030 10214907346980188.0  eT9ecx4odm\n",
      "\n",
      "[81636 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    print(data[~(data.video_id.astype(int)).isin(textdata.video_id.values)][['video_id','creator_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to check the corresponsing creator by video id\n",
    "text_creator = dict(zip(textdata.video_id, textdata.creator_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30263/3289606103.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['new_id'] = data.video_id.astype(np.int64)\n"
     ]
    }
   ],
   "source": [
    "# correcting for video id\n",
    "data['new_id'] = data.video_id.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138397\n"
     ]
    }
   ],
   "source": [
    "# those who match\n",
    "mask = data.new_id.isin(textdata.video_id.values)\n",
    "print(mask.sum())\n",
    "for i,row in data[mask][['video_id','creator_id']].iterrows():\n",
    "    creator = row.creator_id\n",
    "    video_id = row.video_id\n",
    "    if text_creator[video_id]!=creator:\n",
    "        print(f\"video_id: {video_id}, creator_id: {creator}, text_creator: {text_creator[video_id]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one up or one down\n",
    "new_ids = []\n",
    "for i, row in data[~mask][['new_id','creator_id']].iterrows():\n",
    "    creator = row.creator_id\n",
    "    video_id = row.new_id\n",
    "    if text_creator.get(video_id,None)==creator:\n",
    "        raise \"Something wrong happened!\" # shouldn't happen\n",
    "    elif text_creator.get(video_id+1,None)==creator:\n",
    "        new_ids.append(video_id+1)\n",
    "    elif text_creator.get(video_id-1,None)==creator:\n",
    "        new_ids.append(video_id-1)\n",
    "    else:\n",
    "        new_ids.append(video_id)\n",
    "data.loc[~mask,'new_id'] = new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220033"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data.new_id.isin(textdata.video_id.values).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sponsor Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4596\n"
     ]
    }
   ],
   "source": [
    "# calculate creator embeddings\n",
    "document_vectors = model.document_vectors\n",
    "creators = textdata['creator_id'].values\n",
    "\n",
    "# Group document vectors by author\n",
    "creator_to_vectors = defaultdict(list)\n",
    "\n",
    "for creator, vector in zip(creators, document_vectors):\n",
    "    creator_to_vectors[creator].append(vector)\n",
    "\n",
    "creator_embeddings = {}\n",
    "\n",
    "for creator, vectors in creator_to_vectors.items():\n",
    "    # Calculate the mean vector (centroid) for each author\n",
    "    creator_embeddings[creator] = np.mean(vectors, axis=0)\n",
    "    # l2 normalization\n",
    "    creator_embeddings[creator] /= np.linalg.norm(creator_embeddings[creator])\n",
    "\n",
    "print(len(creator_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the creator embeddings\n",
    "creator_embeddings_path = os.path.join(DATA_PATH, 'creator_embeddings.pkl')\n",
    "with open(creator_embeddings_path, 'wb') as f:\n",
    "    pickle.dump(creator_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4596\n"
     ]
    }
   ],
   "source": [
    "# load the creator embeddings\n",
    "creator_embeddings_path = os.path.join(DATA_PATH, 'creator_embeddings.pkl')\n",
    "with open(creator_embeddings_path, 'rb') as f:\n",
    "    creator_embeddings = pickle.load(f)\n",
    "print(len(creator_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "creators_id2name = dict(zip(textdata['creator_id'], textdata['creator_name']))\n",
    "creator_embeddings_names = {}\n",
    "\n",
    "for creator, vectors in creator_embeddings.items():\n",
    "    # two dict by id and name\n",
    "    creator_embeddings_names[creators_id2name[creator]] = vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity for creators\n",
    "def get_similar_creators(creator, creator_embeddings, top_n=5):\n",
    "    creator_embedding = creator_embeddings[creator]\n",
    "    similarities = {}\n",
    "    for key, value in creator_embeddings.items():\n",
    "        similarities[key] = cosine_similarity([creator_embedding], [value])[0][0]\n",
    "    similar_creators = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return similar_creators[:top_n]\n",
    "\n",
    "def print_similar_creators(creator, top_n=5):\n",
    "    similar_creators = get_similar_creators(creator, creator_embeddings_names, top_n)\n",
    "    print(f\"Creators similar to {creator}:\")\n",
    "    for i, (creator, similarity) in enumerate(similar_creators):\n",
    "        print(f\"{i+1:2d}. ({similarity:4.2f}) {creator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creators similar to NFL:\n",
      " 1. (1.00) NFL\n",
      " 2. (0.91) Sunday Night Football on NBC\n",
      " 3. (0.91) ESPN\n",
      " 4. (0.90) NBC Sports\n",
      " 5. (0.89) CBS Sports\n"
     ]
    }
   ],
   "source": [
    "print_similar_creators('NFL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity between two creators\n",
    "def get_sponsorship_similarity(creator1_vector, creator2_embedding):\n",
    "    return cosine_similarity([creator1_vector], [creator2_embedding])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30263/1492927654.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['similarity_agg'] = data.apply(lambda x: new_metric.get((x.creator_id, x.sponsor_id), np.nan), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# calculate similarity between each document and each creator\n",
    "new_metric = {} # key: (creator_id, sponsor_id), value: similarity\n",
    "for i, row in data.drop_duplicates(['creator_id','sponsor_id']).iterrows():\n",
    "    creator_id = row['creator_id']\n",
    "    sponsor_id = row['sponsor_id']\n",
    "    sponsor_name = row['sponsor_name']\n",
    "    creator_name = row['creator_name']\n",
    "    if (sponsor_id not in creator_embeddings) | (creator_id not in creator_embeddings):\n",
    "        if sponsor_name in creator_embeddings_names:\n",
    "            new_metric[(creator_id, sponsor_id)] = get_sponsorship_similarity(creator_embeddings[creator_id], creator_embeddings_names[sponsor_name])\n",
    "        else:\n",
    "            new_metric[(creator_id, sponsor_id)] = np.nan\n",
    "    else:  \n",
    "        new_metric[(creator_id, sponsor_id)] = get_sponsorship_similarity(creator_embeddings[creator_id], creator_embeddings[sponsor_id])\n",
    "\n",
    "# add new metric to the data\n",
    "data['similarity_agg'] = data.apply(lambda x: new_metric.get((x.creator_id, x.sponsor_id), np.nan), axis=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    17355.000000\n",
      "mean         0.540738\n",
      "std          0.163068\n",
      "min         -0.004425\n",
      "25%          0.434317\n",
      "50%          0.541313\n",
      "75%          0.652583\n",
      "max          0.974303\n",
      "Name: similarity_agg, dtype: float64\n",
      "202678\n"
     ]
    }
   ],
   "source": [
    "# explore the new metric\n",
    "print(data.similarity_agg.describe())\n",
    "print(data.similarity_agg.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity        17273\n",
      "similarity_agg    17355\n",
      "dtype: int64\n",
      "                similarity  similarity_agg\n",
      "similarity        1.000000        0.602025\n",
      "similarity_agg    0.602025        1.000000\n",
      "         similarity  similarity_agg\n",
      "count  17273.000000    17355.000000\n",
      "mean       0.683697        0.540738\n",
      "std        0.075628        0.163068\n",
      "min        0.504807       -0.004425\n",
      "25%        0.628146        0.434317\n",
      "50%        0.675194        0.541313\n",
      "75%        0.732961        0.652583\n",
      "max        0.954277        0.974303\n"
     ]
    }
   ],
   "source": [
    "print(data[['similarity','similarity_agg']].notnull().sum())\n",
    "print(data[['similarity','similarity_agg']].corr())\n",
    "print(data[['similarity','similarity_agg']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17355\n"
     ]
    }
   ],
   "source": [
    "# add the column to the data\n",
    "data_jul = pd.read_csv(os.path.join(DATA_PATH, 'pooled_us_jul2024.csv'))\n",
    "#data_jul['similarity_agg'] = data_jul.apply(lambda x: new_metric.get((x.creator_id, x.sponsor_id), np.nan), axis=1)\n",
    "data_jul['similarity_agg'] = data['similarity_agg']\n",
    "# sanity check for the new data (similarity_agg is the same for data and data_jul)\n",
    "print((data.similarity_agg==data_jul.similarity_agg).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "data.to_csv(os.path.join(DATA_PATH,\"pooled_us_aug2024.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctopics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
